{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e474d-967e-4f59-a5d6-85c7b2d10c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\"\"\"\n",
    "Main script that trains, validates, and evaluates\n",
    "various models including AASIST.\n",
    "\n",
    "AASIST\n",
    "Copyright (c) 2021-present NAVER Corp.\n",
    "MIT license\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc9a13-8d42-4aa6-aae5-df115950b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from data_utils import (Dataset_ASVspoof2019_train,\n",
    "                        Dataset_ASVspoof2019_devNeval, genSpoof_list)\n",
    "from evaluation import calculate_tDCF_EER\n",
    "from utils import create_optimizer, seed_worker, set_seed, str_to_bool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972a296-fc2c-4b82-a2ee-08ce265d5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def main(args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    Main function.\n",
    "    Trains, validates, and evaluates the ASVspoof detection model.\n",
    "    \"\"\"\n",
    "    # load experiment configurations\n",
    "    with open(args.config, \"r\") as f_json:\n",
    "        config = json.loads(f_json.read())\n",
    "    model_config = config[\"model_config\"]\n",
    "    optim_config = config[\"optim_config\"]\n",
    "    optim_config[\"epochs\"] = config[\"num_epochs\"]\n",
    "    track = config[\"track\"]\n",
    "    assert track in [\"LA\", \"PA\", \"DF\"], \"Invalid track given\"\n",
    "    if \"eval_all_best\" not in config:\n",
    "        config[\"eval_all_best\"] = \"True\"\n",
    "    if \"freq_aug\" not in config:\n",
    "        config[\"freq_aug\"] = \"False\"\n",
    "\n",
    "    # make experiment reproducible\n",
    "    set_seed(args.seed, config)\n",
    "\n",
    "    # define database related paths\n",
    "    output_dir = Path(args.output_dir)\n",
    "    prefix_2019 = \"ASVspoof2019.{}\".format(track)\n",
    "    database_path = Path(config[\"database_path\"])\n",
    "    dev_trial_path = (database_path /\n",
    "                      \"ASVspoof2019_{}_cm_protocols/{}.cm.dev.trl.txt\".format(\n",
    "                          track, prefix_2019))\n",
    "    eval_trial_path = (\n",
    "        database_path /\n",
    "        \"ASVspoof2019_{}_cm_protocols/{}.cm.eval.trl.txt\".format(\n",
    "            track, prefix_2019))\n",
    "\n",
    "    # define model related paths\n",
    "    model_tag = \"{}_{}_ep{}_bs{}\".format(\n",
    "        track,\n",
    "        os.path.splitext(os.path.basename(args.config))[0],\n",
    "        config[\"num_epochs\"], config[\"batch_size\"])\n",
    "    if args.comment:\n",
    "        model_tag = model_tag + \"_{}\".format(args.comment)\n",
    "    model_tag = output_dir / model_tag\n",
    "    model_save_path = model_tag / \"weights\"\n",
    "    eval_score_path = model_tag / config[\"eval_output\"]\n",
    "    writer = SummaryWriter(model_tag)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    copy(args.config, model_tag / \"config.conf\")\n",
    "\n",
    "    # set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device: {}\".format(device))\n",
    "    # if device == \"cpu\":\n",
    "        # raise ValueError(\"GPU not detected!\")\n",
    "\n",
    "    # define model architecture\n",
    "    model = get_model(model_config, device)\n",
    "\n",
    "    # define dataloaders\n",
    "    trn_loader, dev_loader, eval_loader = get_loader(\n",
    "        database_path, args.seed, config)\n",
    "\n",
    "    # evaluates pretrained model and exit script\n",
    "    if args.eval:\n",
    "        model.load_state_dict(\n",
    "            torch.load(config[\"model_path\"], map_location=device))\n",
    "        print(\"Model loaded : {}\".format(config[\"model_path\"]))\n",
    "        print(\"Start evaluation...\")\n",
    "        produce_evaluation_file(eval_loader, model, device,\n",
    "                                eval_score_path, eval_trial_path)\n",
    "        calculate_tDCF_EER(cm_scores_file=eval_score_path,\n",
    "                           asv_score_file=database_path /\n",
    "                           config[\"asv_score_path\"],\n",
    "                           output_file=model_tag / \"t-DCF_EER.txt\")\n",
    "        print(\"DONE.\")\n",
    "        eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "            cm_scores_file=eval_score_path,\n",
    "            asv_score_file=database_path / config[\"asv_score_path\"],\n",
    "            output_file=model_tag/\"loaded_model_t-DCF_EER.txt\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # get optimizer and scheduler\n",
    "    optim_config[\"steps_per_epoch\"] = len(trn_loader)\n",
    "    optimizer, scheduler = create_optimizer(model.parameters(), optim_config)\n",
    "    optimizer_swa = SWA(optimizer)\n",
    "\n",
    "    best_dev_eer = 1.\n",
    "    best_eval_eer = 100.\n",
    "    best_dev_tdcf = 0.05\n",
    "    best_eval_tdcf = 1.\n",
    "    n_swa_update = 0  # number of snapshots of model to use in SWA\n",
    "    f_log = open(model_tag / \"metric_log.txt\", \"a\")\n",
    "    f_log.write(\"=\" * 5 + \"\\n\")\n",
    "\n",
    "    # make directory for metric logging\n",
    "    metric_path = model_tag / \"metrics\"\n",
    "    os.makedirs(metric_path, exist_ok=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(\"Start training epoch{:03d}\".format(epoch))\n",
    "        running_loss = train_epoch(trn_loader, model, optimizer, device,\n",
    "                                   scheduler, config)\n",
    "        produce_evaluation_file(dev_loader, model, device,\n",
    "                                metric_path/\"dev_score.txt\", dev_trial_path)\n",
    "        dev_eer, dev_tdcf = calculate_tDCF_EER(\n",
    "            cm_scores_file=metric_path/\"dev_score.txt\",\n",
    "            asv_score_file=database_path/config[\"asv_score_path\"],\n",
    "            output_file=metric_path/\"dev_t-DCF_EER_{}epo.txt\".format(epoch),\n",
    "            printout=False)\n",
    "        print(\"DONE.\\nLoss:{:.5f}, dev_eer: {:.3f}, dev_tdcf:{:.5f}\".format(\n",
    "            running_loss, dev_eer, dev_tdcf))\n",
    "        writer.add_scalar(\"loss\", running_loss, epoch)\n",
    "        writer.add_scalar(\"dev_eer\", dev_eer, epoch)\n",
    "        writer.add_scalar(\"dev_tdcf\", dev_tdcf, epoch)\n",
    "\n",
    "        best_dev_tdcf = min(dev_tdcf, best_dev_tdcf)\n",
    "        if best_dev_eer >= dev_eer:\n",
    "            print(\"best model find at epoch\", epoch)\n",
    "            best_dev_eer = dev_eer\n",
    "            torch.save(model.state_dict(),\n",
    "                       model_save_path / \"epoch_{}_{:03.3f}.pth\".format(epoch, dev_eer))\n",
    "\n",
    "            # do evaluation whenever best model is renewed\n",
    "            if str_to_bool(config[\"eval_all_best\"]):\n",
    "                produce_evaluation_file(eval_loader, model, device,\n",
    "                                        eval_score_path, eval_trial_path)\n",
    "                eval_eer, eval_tdcf = calculate_tDCF_EER(\n",
    "                    cm_scores_file=eval_score_path,\n",
    "                    asv_score_file=database_path / config[\"asv_score_path\"],\n",
    "                    output_file=metric_path /\n",
    "                    \"t-DCF_EER_{:03d}epo.txt\".format(epoch))\n",
    "\n",
    "                log_text = \"epoch{:03d}, \".format(epoch)\n",
    "                if eval_eer < best_eval_eer:\n",
    "                    log_text += \"best eer, {:.4f}%\".format(eval_eer)\n",
    "                    best_eval_eer = eval_eer\n",
    "                if eval_tdcf < best_eval_tdcf:\n",
    "                    log_text += \"best tdcf, {:.4f}\".format(eval_tdcf)\n",
    "                    best_eval_tdcf = eval_tdcf\n",
    "                    torch.save(model.state_dict(),\n",
    "                               model_save_path / \"best.pth\")\n",
    "                if len(log_text) > 0:\n",
    "                    print(log_text)\n",
    "                    f_log.write(log_text + \"\\n\")\n",
    "\n",
    "            print(\"Saving epoch {} for swa\".format(epoch))\n",
    "            optimizer_swa.update_swa()\n",
    "            n_swa_update += 1\n",
    "        writer.add_scalar(\"best_dev_eer\", best_dev_eer, epoch)\n",
    "        writer.add_scalar(\"best_dev_tdcf\", best_dev_tdcf, epoch)\n",
    "\n",
    "    print(\"Start final evaluation\")\n",
    "    epoch += 1\n",
    "    if n_swa_update > 0:\n",
    "        optimizer_swa.swap_swa_sgd()\n",
    "        optimizer_swa.bn_update(trn_loader, model, device=device)\n",
    "    produce_evaluation_file(eval_loader, model, device, eval_score_path,\n",
    "                            eval_trial_path)\n",
    "    eval_eer, eval_tdcf = calculate_tDCF_EER(cm_scores_file=eval_score_path,\n",
    "                                             asv_score_file=database_path /\n",
    "                                             config[\"asv_score_path\"],\n",
    "                                             output_file=model_tag / \"t-DCF_EER.txt\")\n",
    "    f_log = open(model_tag / \"metric_log.txt\", \"a\")\n",
    "    f_log.write(\"=\" * 5 + \"\\n\")\n",
    "    f_log.write(\"EER: {:.3f}, min t-DCF: {:.5f}\".format(eval_eer, eval_tdcf))\n",
    "    f_log.close()\n",
    "\n",
    "    torch.save(model.state_dict(),\n",
    "               model_save_path / \"swa.pth\")\n",
    "\n",
    "    if eval_eer <= best_eval_eer:\n",
    "        best_eval_eer = eval_eer\n",
    "    if eval_tdcf <= best_eval_tdcf:\n",
    "        best_eval_tdcf = eval_tdcf\n",
    "        torch.save(model.state_dict(),\n",
    "                   model_save_path / \"best.pth\")\n",
    "    print(\"Exp FIN. EER: {:.3f}, min t-DCF: {:.5f}\".format(\n",
    "        best_eval_eer, best_eval_tdcf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3092488-afd3-4ea5-9b2e-2196b386e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def get_model(model_config: Dict, device: torch.device):\n",
    "    \"\"\"Define DNN model architecture\"\"\"\n",
    "    module = import_module(\"models.{}\".format(model_config[\"architecture\"]))\n",
    "    _model = getattr(module, \"Model\")\n",
    "    model = _model(model_config).to(device)\n",
    "    nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "    print(\"no. model params:{}\".format(nb_params))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eef517-e752-4a06-b679-4bc96cc54d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def get_loader(\n",
    "        database_path: str,\n",
    "        seed: int,\n",
    "        config: dict) -> List[torch.utils.data.DataLoader]:\n",
    "    \"\"\"Make PyTorch DataLoaders for train / developement / evaluation\"\"\"\n",
    "    track = config[\"track\"]\n",
    "    prefix_2019 = \"ASVspoof2019.{}\".format(track)\n",
    "\n",
    "    trn_database_path = database_path / \"ASVspoof2019_{}_train/\".format(track)\n",
    "    dev_database_path = database_path / \"ASVspoof2019_{}_dev/\".format(track)\n",
    "    eval_database_path = database_path / \"ASVspoof2019_{}_eval/\".format(track)\n",
    "\n",
    "    trn_list_path = (database_path /\n",
    "                     \"ASVspoof2019_{}_cm_protocols/{}.cm.train.trn.txt\".format(\n",
    "                         track, prefix_2019))\n",
    "    dev_trial_path = (database_path /\n",
    "                      \"ASVspoof2019_{}_cm_protocols/{}.cm.dev.trl.txt\".format(\n",
    "                          track, prefix_2019))\n",
    "    eval_trial_path = (\n",
    "        database_path /\n",
    "        \"ASVspoof2019_{}_cm_protocols/{}.cm.eval.trl.txt\".format(\n",
    "            track, prefix_2019))\n",
    "\n",
    "    d_label_trn, file_train = genSpoof_list(dir_meta=trn_list_path,\n",
    "                                            is_train=True,\n",
    "                                            is_eval=False)\n",
    "    print(\"no. training files:\", len(file_train))\n",
    "\n",
    "    train_set = Dataset_ASVspoof2019_train(list_IDs=file_train,\n",
    "                                           labels=d_label_trn,\n",
    "                                           base_dir=trn_database_path)\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(seed)\n",
    "    trn_loader = DataLoader(train_set,\n",
    "                            batch_size=config[\"batch_size\"],\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            pin_memory=True,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=gen)\n",
    "\n",
    "    _, file_dev = genSpoof_list(dir_meta=dev_trial_path,\n",
    "                                is_train=False,\n",
    "                                is_eval=False)\n",
    "    print(\"no. validation files:\", len(file_dev))\n",
    "\n",
    "    dev_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_dev,\n",
    "                                            base_dir=dev_database_path)\n",
    "    dev_loader = DataLoader(dev_set,\n",
    "                            batch_size=config[\"batch_size\"],\n",
    "                            shuffle=False,\n",
    "                            drop_last=False,\n",
    "                            pin_memory=True)\n",
    "\n",
    "    file_eval = genSpoof_list(dir_meta=eval_trial_path,\n",
    "                              is_train=False,\n",
    "                              is_eval=True)\n",
    "    eval_set = Dataset_ASVspoof2019_devNeval(list_IDs=file_eval,\n",
    "                                             base_dir=eval_database_path)\n",
    "    eval_loader = DataLoader(eval_set,\n",
    "                             batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,\n",
    "                             drop_last=False,\n",
    "                             pin_memory=True)\n",
    "\n",
    "    return trn_loader, dev_loader, eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92725eb7-3d94-4d28-a827-501a1bed7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def produce_evaluation_file(\n",
    "    data_loader: DataLoader,\n",
    "    model,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    trial_path: str) -> None:\n",
    "    \"\"\"Perform evaluation and save the score to a file\"\"\"\n",
    "    model.eval()\n",
    "    with open(trial_path, \"r\") as f_trl:\n",
    "        trial_lines = f_trl.readlines()\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    for batch_x, utt_id in data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, batch_out = model(batch_x)\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "    assert len(trial_lines) == len(fname_list) == len(score_list)\n",
    "    with open(save_path, \"w\") as fh:\n",
    "        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n",
    "            _, utt_id, _, src, key = trl.strip().split(' ')\n",
    "            assert fn == utt_id\n",
    "            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n",
    "    print(\"Scores saved to {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46732601-02f5-41c0-a2d7-235d253a7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def train_epoch(\n",
    "    trn_loader: DataLoader,\n",
    "    model,\n",
    "    optim: Union[torch.optim.SGD, torch.optim.Adam],\n",
    "    device: torch.device,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    config: argparse.Namespace):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    running_loss = 0\n",
    "    num_total = 0.0\n",
    "    ii = 0\n",
    "    model.train()\n",
    "\n",
    "    # set objective (Loss) functions\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    for batch_x, batch_y in trn_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        ii += 1\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        _, batch_out = model(batch_x, Freq_aug=str_to_bool(config[\"freq_aug\"]))\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "        optim.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if config[\"optim_config\"][\"scheduler\"] in [\"cosine\", \"keras_decay\"]:\n",
    "            scheduler.step()\n",
    "        elif scheduler is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"scheduler error, got:{}\".format(scheduler))\n",
    "\n",
    "    running_loss /= num_total\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250165fe-9c3f-4870-ba76-505cb5e84bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"ASVspoof detection system\")\n",
    "    parser.add_argument(\"--config\",\n",
    "                        dest=\"config\",\n",
    "                        type=str,\n",
    "                        help=\"configuration file\",\n",
    "                        required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        dest=\"output_dir\",\n",
    "        type=str,\n",
    "        help=\"output directory for results\",\n",
    "        default=\"./exp_result\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        type=int,\n",
    "                        default=1234,\n",
    "                        help=\"random seed (default: 1234)\")\n",
    "    parser.add_argument(\n",
    "        \"--eval\",\n",
    "        action=\"store_true\",\n",
    "        help=\"when this flag is given, evaluates given model and exit\")\n",
    "    parser.add_argument(\"--comment\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"comment to describe the saved model\")\n",
    "    parser.add_argument(\"--eval_model_weights\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"directory to the model weight file (can be also given in the config file)\")\n",
    "    main(parser.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
